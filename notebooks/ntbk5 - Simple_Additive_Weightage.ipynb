{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1156464",
   "metadata": {},
   "source": [
    "# Simple Additive Weighting (SAW) Method in Multi-Criteria Decision Making\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "\n",
    "Simple Additive Weighting (SAW) is one of the most widely used Multi-Criteria Decision Making (MCDM) methods. It's also known as the **Weighted Sum Model (WSM)** or **Linear Additive Model**. \n",
    "\n",
    "\n",
    "SAW is particularly useful when you need to:\n",
    "\n",
    "- Evaluate alternatives based on multiple criteria\n",
    "\n",
    "- Handle both quantitative and qualitative data\n",
    "\n",
    "- Make decisions with clear, interpretable results\n",
    "\n",
    "\n",
    "\n",
    "### How SAW Works\n",
    "\n",
    "\n",
    "\n",
    "1. **Normalization**: Convert all criteria to a common scale (0-1)\n",
    "\n",
    "2. **Weighting**: Apply importance weights to each criterion\n",
    "\n",
    "3. **Aggregation**: Sum the weighted normalized values for each alternative\n",
    "\n",
    "4. **Ranking**: Select the alternative with the highest total score\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Steps to Perform SAW\n",
    "\n",
    "\n",
    "1. **Construct the Decision Matrix**: List all alternatives and criteria in a table, with values for each criterion per alternative.\n",
    "2. **Normalize the Decision Matrix**: For each criterion, normalize the values (e.g., for benefit criteria: $x_{ij}' = x_{ij} / \\max(x_j)$; for cost criteria: $x_{ij}' = \\min(x_j) / x_{ij}$).\n",
    "\n",
    "3. **Assign Weights to Criteria**: Determine the relative importance of each criterion and assign a weight (sum of all weights = 1).\n",
    "\n",
    "4. **Calculate Weighted Scores**: Multiply each normalized value by its corresponding criterion weight.\n",
    "5. **Aggregate Scores**: Sum the weighted scores for each alternative to get the final SAW score.\n",
    "6. **Rank the Alternatives**: Rank alternatives based on their SAW scores; the highest score is the best choice.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Let's dive into a practical example!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eee18d3",
   "metadata": {},
   "source": [
    "## SAW Process Flow\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[Construct Decision Matrix<br/>List alternatives and criteria] --> B[Normalize Decision Matrix<br/>Scale values to 0-1 range]\n",
    "    B --> C[Assign Weights to Criteria<br/>Define importance of each criterion]\n",
    "    C --> D[Calculate Weighted Scores<br/>Multiply normalized values by weights]\n",
    "    D --> E[Aggregate Scores<br/>Sum weighted scores for each alternative]\n",
    "    E --> F[Rank Alternatives<br/>Select highest scoring alternative]\n",
    "    \n",
    "    style A fill:#e1f5fe\n",
    "    style B fill:#f3e5f5\n",
    "    style C fill:#e8f5e8\n",
    "    style D fill:#fff3e0\n",
    "    style E fill:#fce4ec\n",
    "    style F fill:#e8eaf6\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6598aff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set display options for better readability\n",
    "pd.set_option('display.precision', 4)\n",
    "pd.set_option('display.width', None)\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64ec7dc",
   "metadata": {},
   "source": [
    "## Problem Statement: Selecting the Best Laptop\n",
    "\n",
    "Imagine you're shopping for a laptop and need to choose between 5 different models. You want to evaluate them based on multiple criteria:\n",
    "\n",
    "- **Price** (lower is better) - Cost consideration\n",
    "- **Performance Score** (higher is better) - Processing power\n",
    "- **Battery Life** (higher is better) - Hours of usage\n",
    "- **Weight** (lower is better) - Portability\n",
    "- **Screen Quality** (higher is better) - Display rating (1-10)\n",
    "\n",
    "Each criterion has different importance to you, and we'll use SAW to find the best overall choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9854f9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the decision matrix (alternatives vs criteria)\n",
    "laptops_data = {\n",
    "    'Laptop': ['Laptop A', 'Laptop B', 'Laptop C', 'Laptop D', 'Laptop E'],\n",
    "    'Price ($)': [800, 1200, 1000, 1500, 900],\n",
    "    'Performance Score': [75, 90, 85, 95, 80],\n",
    "    'Battery Life (hrs)': [8, 6, 10, 7, 12],\n",
    "    'Weight (kg)': [2.5, 1.8, 2.2, 2.0, 2.8],\n",
    "    'Screen Quality (1-10)': [7, 9, 8, 10, 6]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(laptops_data)\n",
    "print(\"Decision Matrix:\")\n",
    "print(\"=\"*50)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Show data types and basic info\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Alternatives: {len(df)} laptops\")\n",
    "print(f\"Criteria: {len(df.columns)-1} criteria\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad83779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define criteria weights (importance of each criterion)\n",
    "# Weights should sum to 1.0\n",
    "criteria_weights = {\n",
    "    'Price ($)': 0.25,            # 25% importance\n",
    "    'Performance Score': 0.30,    # 30% importance  \n",
    "    'Battery Life (hrs)': 0.20,   # 20% importance\n",
    "    'Weight (kg)': 0.15,          # 15% importance\n",
    "    'Screen Quality (1-10)': 0.10  # 10% importance\n",
    "}\n",
    "\n",
    "# Convert to numpy array for calculations\n",
    "weights = np.array(list(criteria_weights.values()))\n",
    "\n",
    "print(\"Criteria Weights:\")\n",
    "print(\"=\"*30)\n",
    "for criterion, weight in criteria_weights.items():\n",
    "    print(f\"{criterion:<20}: {weight:.2f} ({weight*100:.0f}%)\")\n",
    "\n",
    "print(f\"\\nTotal weight sum: {weights.sum():.2f}\")\n",
    "\n",
    "# Verify weights sum to 1\n",
    "if abs(weights.sum() - 1.0) < 0.001:\n",
    "    print(\"Weights sum to 1.0\")\n",
    "else:\n",
    "    print(\"Warning: Weights do not sum to 1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14356e6",
   "metadata": {},
   "source": [
    "## Step 1: Normalization\n",
    "\n",
    "Before applying weights, we need to normalize the criteria values to a common scale (0-1). There are different normalization methods:\n",
    "\n",
    "### 1. **Min-Max Normalization**\n",
    "- **For Benefit criteria** (higher is better): $r_{ij} = \\frac{x_{ij} - \\min_j(x_{ij})}{\\max_j(x_{ij}) - \\min_j(x_{ij})}$\n",
    "- **For Cost criteria** (lower is better): $r_{ij} = \\frac{\\max_j(x_{ij}) - x_{ij}}{\\max_j(x_{ij}) - \\min_j(x_{ij})}$\n",
    "\n",
    "### 2. **Linear Scale Transformation**\n",
    "- **For Benefit criteria**: $r_{ij} = \\frac{x_{ij}}{\\max_j(x_{ij})}$\n",
    "- **For Cost criteria**: $r_{ij} = \\frac{\\min_j(x_{ij})}{x_{ij}}$\n",
    "\n",
    "We'll use **Min-Max normalization** as it's the most intuitive and commonly used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adcb34c",
   "metadata": {},
   "source": [
    "Both Min-Max Normalization and Linear Scale Transformation are used to bring different criteria onto a common scale, but they do so differently and are suitable for different situations.\n",
    "\n",
    "### Min-Max Normalization\n",
    "\n",
    "This is the most common and generally recommended method for SAW. It scales every value to a fixed range of [0, 1], where the best-performing alternative for a criterion gets a score of 1 and the worst gets a score of 0.\n",
    "\n",
    "*   **When to use:**\n",
    "    *   When you want a true relative comparison where the full range of performance (from worst to best) is captured.\n",
    "    *   When you want all normalized criteria to be on the exact same [0, 1] scale.\n",
    "    *   It is the standard and most intuitive method for SAW.\n",
    "\n",
    "### Linear Scale Transformation\n",
    "\n",
    "This method scales values relative to a single pointâ€”either the maximum value (for benefit criteria) or the minimum value (for cost criteria). The range is not strictly [0, 1].\n",
    "\n",
    "*   **When to use:**\n",
    "    *   When you want to evaluate performance as a proportion of the \"best\" available option. For example, for a benefit criterion, a value is judged as a percentage of the maximum observed value.\n",
    "    *   When the absolute minimum is not a meaningful baseline.\n",
    "\n",
    "### What to Be Careful About\n",
    "\n",
    "1.  **Outliers (Critical for Min-Max):** Min-Max normalization is very sensitive to outliers. A single extreme value (either very high or very low) will become the `max` or `min` in the formula, which can squash all the other \"normal\" data points into a very small range, reducing their differentiation.\n",
    "\n",
    "2.  **Division by Zero (Critical for Linear Scale):** The cost formula for Linear Scale Transformation is `min(x) / x`. If any alternative has a value of `0` for a cost criterion, this will result in a division-by-zero error.\n",
    "\n",
    "3.  **Interpretation:** The two methods produce different normalized values, which can lead to different final rankings. Min-Max measures where a value falls within the full observed range, while Linear Scale measures it relative to the best value. Be consistent and understand which interpretation fits your problem better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9e04b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define normalization functions\n",
    "def normalize_benefit_criterion(values):\n",
    "    \"\"\"\n",
    "    Normalize benefit criterion (higher is better)\n",
    "    Formula: (value - min) / (max - min)\n",
    "    \"\"\"\n",
    "    min_val = np.min(values)\n",
    "    max_val = np.max(values)\n",
    "    if max_val == min_val:\n",
    "        return np.ones_like(values)  # All values are the same\n",
    "    return (values - min_val) / (max_val - min_val)\n",
    "\n",
    "def normalize_cost_criterion(values):\n",
    "    \"\"\"\n",
    "    Normalize cost criterion (lower is better)\n",
    "    Formula: (max - value) / (max - min)\n",
    "    \"\"\"\n",
    "    min_val = np.min(values)\n",
    "    max_val = np.max(values)\n",
    "    if max_val == min_val:\n",
    "        return np.ones_like(values)  # All values are the same\n",
    "    return (max_val - values) / (max_val - min_val)\n",
    "\n",
    "# Define criterion types (benefit or cost)\n",
    "criterion_types = {\n",
    "    'Price ($)': 'cost',          # Lower price is better\n",
    "    'Performance Score': 'benefit',  # Higher performance is better\n",
    "    'Battery Life (hrs)': 'benefit', # Longer battery life is better\n",
    "    'Weight (kg)': 'cost',           # Lower weight is better\n",
    "    'Screen Quality (1-10)': 'benefit' # Higher quality is better\n",
    "}\n",
    "\n",
    "print(\"Criterion Types:\")\n",
    "print(\"=\"*25)\n",
    "for criterion, type_ in criterion_types.items():\n",
    "    print(f\"{criterion:<20}: {type_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f452ebd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply normalization to each criterion\n",
    "normalized_data = df.copy()\n",
    "\n",
    "print(\"Normalization Process:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for criterion in criterion_types.keys():\n",
    "    values = df[criterion].values\n",
    "    \n",
    "    print(f\"\\n{criterion}:\")\n",
    "    print(f\"  Original values: {values}\")\n",
    "    print(f\"  Type: {criterion_types[criterion]}\")\n",
    "    \n",
    "    if criterion_types[criterion] == 'benefit':\n",
    "        normalized_values = normalize_benefit_criterion(values)\n",
    "    else:\n",
    "        normalized_values = normalize_cost_criterion(values)\n",
    "    \n",
    "    normalized_data[criterion] = normalized_values\n",
    "    print(f\"  Normalized: {normalized_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c93e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"NORMALIZED DECISION MATRIX:\")\n",
    "print(\"=\"*50)\n",
    "print(normalized_data.to_string(index=False, float_format='%.4f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8944b38",
   "metadata": {},
   "source": [
    "## Step 2: Apply SAW Formula\n",
    "\n",
    "The SAW method calculates the final score for each alternative using:\n",
    "\n",
    "$$S_i = \\sum_{j=1}^{n} w_j \\times r_{ij}$$\n",
    "\n",
    "Where:\n",
    "- $S_i$ = Total score for alternative $i$\n",
    "- $w_j$ = Weight of criterion $j$\n",
    "- $r_{ij}$ = Normalized value of alternative $i$ for criterion $j$\n",
    "- $n$ = Number of criteria\n",
    "\n",
    "The alternative with the **highest score** is the best choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5a36cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SAW scores\n",
    "def calculate_saw_scores(normalized_matrix, weights):\n",
    "    \"\"\"\n",
    "    Calculate SAW scores for each alternative\n",
    "    \"\"\"\n",
    "    # Extract only the criteria columns (exclude 'Laptop' column)\n",
    "    criteria_columns = [col for col in normalized_matrix.columns if col != 'Laptop']\n",
    "    criteria_data = normalized_matrix[criteria_columns].values\n",
    "    \n",
    "    # Calculate weighted sum for each alternative\n",
    "    scores = np.dot(criteria_data, weights)\n",
    "    return scores\n",
    "\n",
    "# Get normalized criteria data\n",
    "criteria_columns = [col for col in normalized_data.columns if col != 'Laptop']\n",
    "normalized_matrix = normalized_data[criteria_columns].values\n",
    "\n",
    "print(\"Calculation Details:\")\n",
    "print(\"=\"*50)\n",
    "print(\"Normalized Matrix (criteria only):\")\n",
    "print(normalized_matrix)\n",
    "print(f\"\\nWeights: {weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01bf4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SAW scores\n",
    "saw_scores = calculate_saw_scores(normalized_data, weights)\n",
    "\n",
    "# Create results dataframe\n",
    "results = pd.DataFrame({\n",
    "    'Laptop': df['Laptop'],\n",
    "    'SAW Score': saw_scores\n",
    "})\n",
    "\n",
    "# Add detailed breakdown\n",
    "print(f\"\\nSAW Score Calculation:\")\n",
    "print(\"=\"*30)\n",
    "for i, laptop in enumerate(df['Laptop']):\n",
    "    print(f\"\\n{laptop}:\")\n",
    "    total = 0\n",
    "    for j, criterion in enumerate(criteria_columns):\n",
    "        contribution = normalized_matrix[i, j] * weights[j]\n",
    "        total += contribution\n",
    "        print(f\"  {criterion:<20}: {normalized_matrix[i, j]:.4f} Ã— {weights[j]:.3f} = {contribution:.4f}\")\n",
    "    print(f\"  {'Total SAW Score':<20}: {total:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e663879",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*50}\")\n",
    "print(\"FINAL SAW SCORES:\")\n",
    "print(\"=\"*50)\n",
    "results_sorted = results.sort_values('SAW Score', ascending=False)\n",
    "print(results_sorted.to_string(index=False, float_format='%.4f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c33dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ranking to results\n",
    "results_sorted['Rank'] = range(1, len(results_sorted) + 1)\n",
    "\n",
    "print(\"FINAL RANKING:\")\n",
    "print(\"=\"*40)\n",
    "for idx, row in results_sorted.iterrows():\n",
    "    print(f\"{row['Rank']}. {row['Laptop']:<10} - Score: {row['SAW Score']:.4f}\")\n",
    "\n",
    "# Best choice\n",
    "best_laptop = results_sorted.iloc[0]\n",
    "print(f\"\\nBEST CHOICE: {best_laptop['Laptop']}\")\n",
    "print(f\"SAW Score: {best_laptop['SAW Score']:.4f}\")\n",
    "\n",
    "# Show percentage scores\n",
    "print(f\"\\nPERCENTAGE SCORES:\")\n",
    "print(\"=\"*30)\n",
    "for idx, row in results_sorted.iterrows():\n",
    "    percentage = (row['SAW Score'] / results_sorted['SAW Score'].max()) * 100\n",
    "    print(f\"{row['Rank']}. {row['Laptop']:<10} - {percentage:.1f}%\")\n",
    "\n",
    "# Performance gap analysis\n",
    "print(f\"\\nPERFORMANCE GAPS:\")\n",
    "print(\"=\"*35)\n",
    "best_score = results_sorted['SAW Score'].max()\n",
    "for idx, row in results_sorted.iterrows():\n",
    "    if row['Rank'] > 1:\n",
    "        gap = best_score - row['SAW Score']\n",
    "        gap_percentage = (gap / best_score) * 100\n",
    "        print(f\"{row['Laptop']:<10} - Gap: {gap:.4f} ({gap_percentage:.1f}% behind leader)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5db3ceb",
   "metadata": {},
   "source": [
    "## Step 3: Visualization\n",
    "\n",
    "Let's create visualizations to better understand our results and the decision-making process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6c2674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Simple Additive Weighting (SAW) Analysis Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. SAW Scores Bar Chart\n",
    "ax1 = axes[0, 0]\n",
    "bars = ax1.bar(results_sorted['Laptop'], results_sorted['SAW Score'], \n",
    "               color=['gold' if i == 0 else 'lightblue' for i in range(len(results_sorted))])\n",
    "ax1.set_title('SAW Scores by Laptop', fontweight='bold')\n",
    "ax1.set_ylabel('SAW Score')\n",
    "ax1.set_ylim(0, results_sorted['SAW Score'].max() * 1.1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars, results_sorted['SAW Score']):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 2. Criteria Weights Pie Chart\n",
    "ax2 = axes[0, 1]\n",
    "criteria_labels = list(criteria_weights.keys())\n",
    "criteria_values = list(criteria_weights.values())\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(criteria_labels)))\n",
    "\n",
    "wedges, texts, autotexts = ax2.pie(criteria_values, labels=criteria_labels, autopct='%1.1f%%',\n",
    "                                   colors=colors, startangle=90)\n",
    "ax2.set_title('Criteria Weights Distribution', fontweight='bold')\n",
    "\n",
    "# 3. Normalized Values Heatmap\n",
    "ax3 = axes[1, 0]\n",
    "criteria_only_data = normalized_data.drop('Laptop', axis=1)\n",
    "im = ax3.imshow(criteria_only_data.values, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "ax3.set_xticks(range(len(criteria_only_data.columns)))\n",
    "ax3.set_xticklabels(criteria_only_data.columns, rotation=45, ha='right')\n",
    "ax3.set_yticks(range(len(normalized_data)))\n",
    "ax3.set_yticklabels(normalized_data['Laptop'])\n",
    "ax3.set_title('Normalized Criteria Values Heatmap', fontweight='bold')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax3, shrink=0.8)\n",
    "cbar.set_label('Normalized Value (0-1)')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(normalized_data)):\n",
    "    for j in range(len(criteria_only_data.columns)):\n",
    "        text = ax3.text(j, i, f'{criteria_only_data.iloc[i, j]:.2f}',\n",
    "                       ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n",
    "\n",
    "# 4. Ranking Comparison\n",
    "ax4 = axes[1, 1]\n",
    "y_pos = np.arange(len(results_sorted))\n",
    "bars = ax4.barh(y_pos, results_sorted['SAW Score'], \n",
    "                color=['gold' if i == 0 else 'lightcoral' for i in range(len(results_sorted))])\n",
    "ax4.set_yticks(y_pos)\n",
    "ax4.set_yticklabels(results_sorted['Laptop'])\n",
    "ax4.set_xlabel('SAW Score')\n",
    "ax4.set_title('Final Ranking (Horizontal)', fontweight='bold')\n",
    "ax4.invert_yaxis()  # Best on top\n",
    "\n",
    "# Add rank labels\n",
    "for i, (bar, score) in enumerate(zip(bars, results_sorted['SAW Score'])):\n",
    "    width = bar.get_width()\n",
    "    ax4.text(width + 0.01, bar.get_y() + bar.get_height()/2.,\n",
    "             f'#{i+1}', ha='left', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary Statistics\n",
    "print(\"SUMMARY STATISTICS:\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Score Range: {results_sorted['SAW Score'].min():.4f} - {results_sorted['SAW Score'].max():.4f}\")\n",
    "print(f\"Score Spread: {results_sorted['SAW Score'].max() - results_sorted['SAW Score'].min():.4f}\")\n",
    "print(f\"Average Score: {results_sorted['SAW Score'].mean():.4f}\")\n",
    "print(f\"Standard Deviation: {results_sorted['SAW Score'].std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739b3400",
   "metadata": {},
   "source": [
    "## Step 4: Sensitivity Analysis\n",
    "\n",
    "Let's perform a sensitivity analysis to see how changes in weights affect the ranking. This helps us understand the robustness of our decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f18960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitivity Analysis: What if we change the weight of Performance?\n",
    "def sensitivity_analysis(base_weights, criterion_index, weight_range):\n",
    "    \"\"\"\n",
    "    Perform sensitivity analysis by varying a single criterion's weight.\n",
    "\n",
    "    For each candidate weight in weight_range this function:\n",
    "      - creates a test weight vector by setting the selected criterion to the candidate weight,\n",
    "      - proportionally redistributes the remaining weight (1 - candidate) across the other criteria,\n",
    "      - computes SAW scores using the global `normalized_data` and the existing `calculate_saw_scores` function,\n",
    "      - records the winner (alternative with highest SAW score) and the full score vector.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_weights : array-like of float\n",
    "        Original weights for all criteria (expected to sum approximately to 1.0).\n",
    "    criterion_index : int\n",
    "        Zero-based index of the criterion whose weight will be varied.\n",
    "    weight_range : iterable of float\n",
    "        Sequence of candidate weights to try for the selected criterion (each value must be in [0, 1]).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of dictionaries\n",
    "        Each dictionary corresponds to one tested weight and contains:\n",
    "          - 'Performance Weight' : float  (the tested weight value)\n",
    "          - 'Winner'             : str    (name of the winning alternative)\n",
    "          - 'Winning Score'      : float  (SAW score of the winner)\n",
    "          - 'All Scores'         : numpy.ndarray (SAW scores for all alternatives, same order as df['Laptop'])\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Redistribution of remaining weight is proportional to the current weights of the other criteria.\n",
    "      If the sum of other weights is zero, other weights remain zero and the selected weight is used as-is.\n",
    "    - The function does not modify `base_weights`; it works on copies.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    sensitivity_results = sensitivity_analysis(weights, performance_index, np.arange(0.1, 0.6, 0.05))\n",
    "    \"\"\"\n",
    "    results_list = []\n",
    "\n",
    "    # Basic validation\n",
    "    n = len(base_weights)\n",
    "    if not (0 <= criterion_index < n):\n",
    "        raise IndexError(f\"criterion_index {criterion_index} out of range for weights length {n}\")\n",
    "\n",
    "    for new_weight in weight_range:\n",
    "        if not (0.0 <= new_weight <= 1.0):\n",
    "            raise ValueError(f\"Test weight must be in [0, 1], got {new_weight}\")\n",
    "\n",
    "        # Create new weight vector\n",
    "        test_weights = base_weights.copy()\n",
    "        test_weights[criterion_index] = new_weight\n",
    "\n",
    "        # Redistribute remaining weight proportionally\n",
    "        remaining_weight = 1.0 - new_weight\n",
    "        other_indices = [i for i in range(len(test_weights)) if i != criterion_index]\n",
    "        current_other_sum = sum(test_weights[i] for i in other_indices)\n",
    "\n",
    "        if current_other_sum > 0:\n",
    "            for i in other_indices:\n",
    "                test_weights[i] = test_weights[i] * (remaining_weight / current_other_sum)\n",
    "\n",
    "        # Calculate SAW scores with new weights\n",
    "        test_scores = calculate_saw_scores(normalized_data, test_weights)\n",
    "\n",
    "        # Get ranking\n",
    "        test_results = pd.DataFrame({\n",
    "            'Laptop': df['Laptop'],\n",
    "            'SAW Score': test_scores\n",
    "        }).sort_values('SAW Score', ascending=False)\n",
    "\n",
    "        results_list.append({\n",
    "            'Performance Weight': float(new_weight),\n",
    "            'Winner': test_results.iloc[0]['Laptop'],\n",
    "            'Winning Score': float(test_results.iloc[0]['SAW Score']),\n",
    "            'All Scores': test_scores\n",
    "        })\n",
    "\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38936db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform sensitivity analysis on Performance Score weight\n",
    "performance_index = 1  # Performance Score is at index 1\n",
    "weight_range = np.arange(0.1, 0.6, 0.05)  # From 10% to 55% in 5% steps\n",
    "\n",
    "sensitivity_results = sensitivity_analysis(weights, performance_index, weight_range)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64182ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SENSITIVITY ANALYSIS: Performance Score Weight\")\n",
    "print(\"=\"*50)\n",
    "print(\"Testing how changes in Performance weight affect the winner.\")\n",
    "\n",
    "# Display results\n",
    "winners = {}\n",
    "for result in sensitivity_results:\n",
    "    perf_weight = result['Performance Weight']\n",
    "    winner = result['Winner']\n",
    "    winning_score = result['Winning Score']\n",
    "    \n",
    "    if winner not in winners:\n",
    "        winners[winner] = []\n",
    "    winners[winner].append(perf_weight)\n",
    "    \n",
    "    print(f\"Performance Weight: {perf_weight:.2f} ({perf_weight*100:.0f}%) -> Winner: {winner} (Score: {winning_score:.4f})\")\n",
    "\n",
    "print(f\"\\nWINNER SUMMARY:\")\n",
    "print(\"=\"*30)\n",
    "for laptop, weight_ranges in winners.items():\n",
    "    print(f\"{laptop}: Wins when Performance weight is {min(weight_ranges):.2f}-{max(weight_ranges):.2f}\")\n",
    "\n",
    "# Visualize sensitivity analysis\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot scores vs performance weight for each laptop\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "for i, laptop in enumerate(df['Laptop']):\n",
    "    scores = [result['All Scores'][i] for result in sensitivity_results]\n",
    "    plt.plot(weight_range, scores, marker='o', label=laptop, color=colors[i], linewidth=2)\n",
    "\n",
    "plt.axvline(x=weights[performance_index], color='black', linestyle='--', \n",
    "           label=f'Current Weight ({weights[performance_index]:.2f})', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Performance Score Weight')\n",
    "plt.ylabel('SAW Score')\n",
    "plt.title('Sensitivity Analysis: SAW Scores vs Performance Weight')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check stability around current weight\n",
    "current_weight = weights[performance_index]\n",
    "tolerance = 0.05\n",
    "stable_range = [w for w in weight_range if abs(w - current_weight) <= tolerance]\n",
    "stable_winners = [result['Winner'] for result in sensitivity_results \n",
    "                 if result['Performance Weight'] in stable_range]\n",
    "\n",
    "print(f\"\\nSTABILITY ANALYSIS:\")\n",
    "print(\"=\"*30)\n",
    "print(f\"Current Performance weight: {current_weight:.2f}\")\n",
    "print(f\"Within Â±{tolerance:.2f} range:\")\n",
    "if len(set(stable_winners)) == 1:\n",
    "    print(f\"Stable winner: {stable_winners[0]}\")\n",
    "else:\n",
    "    print(f\"Winner changes: {set(stable_winners)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83428e9",
   "metadata": {},
   "source": [
    "## SAW Method: Advantages and Disadvantages\n",
    "\n",
    "### Advantages\n",
    "\n",
    "1. **Simplicity**: Easy to understand and implement\n",
    "2. **Intuitive**: Linear aggregation is natural for decision makers\n",
    "3. **Transparency**: Clear how each criterion contributes to final score\n",
    "4. **Computational Efficiency**: Fast calculation even for large problems\n",
    "5. **Flexibility**: Works with both quantitative and qualitative criteria\n",
    "6. **Wide Applicability**: Suitable for various decision-making contexts\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "1. **Assumption of Additivity**: Assumes criteria are independent (no interactions)\n",
    "2. **Compensation**: Poor performance in one criterion can be completely offset by good performance in others\n",
    "3. **Weight Sensitivity**: Results can be sensitive to weight changes\n",
    "4. **Normalization Dependency**: Different normalization methods can yield different results\n",
    "5. **No Uncertainty Handling**: Doesn't account for uncertainty in data or preferences\n",
    "6. **Linear Relationships**: Assumes linear relationships between criteria and utility\n",
    "\n",
    "### When to Use SAW\n",
    "\n",
    "**Best for:**\n",
    "- Simple decision problems with independent criteria\n",
    "- When compensation between criteria is acceptable\n",
    "- Situations requiring transparent, explainable decisions\n",
    "- Problems with well-defined, measurable criteria\n",
    "\n",
    "**Avoid when:**\n",
    "- Criteria have complex interactions\n",
    "- Compensation is not acceptable (e.g., safety-critical decisions)\n",
    "- High uncertainty in data or preferences\n",
    "- Non-linear relationships between criteria and utility\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Normalization is Critical**: Choose appropriate normalization method for your data\n",
    "2. **Weight Carefully**: Ensure weights truly reflect importance and sum to 1.0\n",
    "3. **Check Sensitivity**: Test how weight changes affect results\n",
    "4. **Validate Results**: Compare with other MCDM methods when possible\n",
    "5. **Consider Context**: Ensure SAW assumptions fit your decision problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952246f0",
   "metadata": {},
   "source": [
    "## Practice Exercise\n",
    "\n",
    "Try implementing SAW for a different scenario! Here's a dataset for choosing a smartphone:\n",
    "\n",
    "**Criteria:**\n",
    "- **Price** (cost criterion): Lower is better\n",
    "- **Camera Quality** (benefit criterion): Higher score is better (1-10)\n",
    "- **Battery Capacity** (benefit criterion): mAh, higher is better  \n",
    "- **Storage** (benefit criterion): GB, higher is better\n",
    "- **Brand Reputation** (benefit criterion): Rating 1-10, higher is better\n",
    "\n",
    "**Your Task:**\n",
    "1. Create the decision matrix\n",
    "2. Define your own weights based on your preferences\n",
    "3. Apply the SAW method\n",
    "4. Interpret the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a1a3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRACTICE: Smartphone Selection using SAW\n",
    "# Complete this exercise to practice SAW implementation\n",
    "\n",
    "# Step 1: Create your decision matrix\n",
    "smartphone_data = {\n",
    "    'Phone': ['iPhone 15', 'Samsung S24', 'Google Pixel 8', 'OnePlus 12', 'Xiaomi 14'],\n",
    "    'Price ($)': [999, 899, 699, 749, 599],\n",
    "    'Camera Quality (1-10)': [9, 8, 9, 7, 8],\n",
    "    'Battery Capacity (mAh)': [3349, 4000, 4575, 5400, 4610],\n",
    "    'Storage (GB)': [256, 256, 128, 256, 512],\n",
    "    'Brand Reputation (1-10)': [10, 9, 8, 7, 6]\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdcm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
